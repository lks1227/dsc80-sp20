{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 7\n",
    "\n",
    "### Due Friday May 15, 11:59:59PM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lecture Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import disc07 as disc\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Review: HTTP Requests and HTML\n",
    "\n",
    "### Requests\n",
    "\n",
    "* HTTP requests can retrieve data using `GET` requests.\n",
    "* The python function `resp = requests.get(url)` returns a response object:\n",
    "    - `resp.text` contains the body of the response (e.g. HTML)\n",
    "    - `resp.status_code` contains the status code of the response (e.g. 200, which means 'ok')\n",
    "    - `resp.body` contains the entire response content.\n",
    "    \n",
    "### Parsing HTML\n",
    "\n",
    "* An HTML page may be described as a tree (Document Object Model)\n",
    "    - The nodes are HTML tags that define regions in the page (i.e. `<tag>...</tag>`).\n",
    "    - Two nodes are connected by an edge if one HTML tag is nested in the other.\n",
    "    - The sub-tree of below a given node (HTML tag) consists of the portion of the HTML page contained within that HTML tag.\n",
    "* `BeautifulSoup` parses an HTML document into its DOM (tree structure).\n",
    "    - `.children` attribute of a node iterates through the sub-trees of the DOM.\n",
    "    - `.find(tag)` method selects the sub-tree of the DOM that contains the specified tag-type.\n",
    "    - `.attr` accesses the attributes of a given tag (e.g. the hyperlink reference).\n",
    "    - `.text` accesses the text between the start (`<tag>`) and end (`</tag>`) tags for a given node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Countries\n",
    "\n",
    "\n",
    "In this question you need to scrape the website `http://example.webscraping.com/` and collect all the countries countries listed on the site (not just the first page). \n",
    "\n",
    "*Side note:* We saw in the lecture that there is a method `pd.read_html` that allows you to read HTML tables into a list of DataFrame objects. You can test it out but please **DO NOT** use it in your solution. The purpose of this problem is for you to practice scraping using simple tags first, before you move on to more difficult problems. \n",
    "\n",
    "### The Plan\n",
    "\n",
    "To get this information of the website, you'll go through the following steps:\n",
    "1. Write a function to *sucessfully* request the website content of a single page.\n",
    "1. Write a function to request the content of all relevant pages.\n",
    "1. Write a function to parse the HTML and extract the countries of an already-requested page.\n",
    "1. Put it all together.\n",
    "\n",
    "Remember, you should always make as few requests as possible. For example, when developing your parsing logic, you should work with a single successful request already on your computer -- don't request the content from the website repeatedly! Breaking your work up into function as specified below will help you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Check the website first, count the number of pages you need to go over, look at the URL of each page. Write a list of URLs that you need to request (find a pattern and use code; do not copy and paste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP temporarily blocked\n"
     ]
    }
   ],
   "source": [
    "url_list=['http://example.webscraping.com/']\n",
    "st = ''\n",
    "new_endpoint = 'http://example.webscraping.com/'\n",
    "url = new_endpoint+st\n",
    "request = requests.get(url)\n",
    "urlText = urlText = request.text\n",
    "soup = bs4.BeautifulSoup(urlText, 'html.parser')\n",
    "for i in soup.find_all('div'):\n",
    "    url = new_endpoint+st\n",
    "    request = requests.get(url)\n",
    "    urlText = urlText = request.text\n",
    "    soup = bs4.BeautifulSoup(urlText, 'html.parser')\n",
    "    att = i.attrs\n",
    "    if i.find(attrs={'id': 'pagination'})!= None:\n",
    "        check = i.find(attrs={'id': 'pagination'}).find('a').text\n",
    "        tag = i.find(attrs={'id': 'pagination'})\n",
    "        if ('Next' in check):\n",
    "            st = tag.find('a').get('href')\n",
    "            url=new_endpoint+st\n",
    "            url_list.append(url)\n",
    "        else:\n",
    "            print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Once you have an idea how the site works, you can start sending requests to the pages and collect results. Loop through the `url_list`, requesting each page, and collect the HTTP responses in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://example.webscraping.com/places/default/index/0',\n",
       " 'http://example.webscraping.com/places/default/index/1',\n",
       " 'http://example.webscraping.com/places/default/index/2',\n",
       " 'http://example.webscraping.com/places/default/index/3',\n",
       " 'http://example.webscraping.com/places/default/index/4',\n",
       " 'http://example.webscraping.com/places/default/index/5',\n",
       " 'http://example.webscraping.com/places/default/index/6',\n",
       " 'http://example.webscraping.com/places/default/index/7',\n",
       " 'http://example.webscraping.com/places/default/index/8',\n",
       " 'http://example.webscraping.com/places/default/index/9',\n",
       " 'http://example.webscraping.com/places/default/index/10',\n",
       " 'http://example.webscraping.com/places/default/index/11',\n",
       " 'http://example.webscraping.com/places/default/index/12',\n",
       " 'http://example.webscraping.com/places/default/index/13',\n",
       " 'http://example.webscraping.com/places/default/index/14',\n",
       " 'http://example.webscraping.com/places/default/index/15',\n",
       " 'http://example.webscraping.com/places/default/index/16',\n",
       " 'http://example.webscraping.com/places/default/index/17',\n",
       " 'http://example.webscraping.com/places/default/index/18',\n",
       " 'http://example.webscraping.com/places/default/index/19',\n",
       " 'http://example.webscraping.com/places/default/index/20',\n",
       " 'http://example.webscraping.com/places/default/index/21',\n",
       " 'http://example.webscraping.com/places/default/index/22',\n",
       " 'http://example.webscraping.com/places/default/index/23',\n",
       " 'http://example.webscraping.com/places/default/index/24',\n",
       " 'http://example.webscraping.com/places/default/index/25']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc.url_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-75-18b996df8986>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-75-18b996df8986>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    return url_list\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "soup = bs4.BeautifulSoup(urlText, 'html.parser')\n",
    "for i in soup.find_all('div'):\n",
    "    att = i.attrs\n",
    "    if i.find(attrs={'id': 'pagination'})!= None:\n",
    "        check = i.find(attrs={'id': 'pagination'}).find('a').text\n",
    "        tag = i.find(attrs={'id': 'pagination'})\n",
    "        if ('Next' in check):\n",
    "            st = tag.find('a').get('href')\n",
    "            url=new_endpoint+st\n",
    "        else:\n",
    "            return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://example.webscraping.com/places/default/index/26/places/default/index/25'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = bs4.BeautifulSoup(urlText, 'html.parser')\n",
    "for i in soup.find_all('div'):\n",
    "    att = i.attrs\n",
    "    if i.find(attrs={'id': 'pagination'})!= None:\n",
    "        tag = i.find(attrs={'id': 'pagination'})\n",
    "        if ('Next' in tag.text):\n",
    "            st = tag.find('a').get('href')\n",
    "            url=new_endpoint+st\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** What happens when you try execute the loop above? Why do you get an exception? You need to modify your code to both (1) handle these exceptions and (2) still collect all the data on the website(s).\n",
    "\n",
    "To do this, create a function `request_until_successful` that takes in a `url` (and any optional keywords you might find useful) and the number of re-try requests the function will attempt `N` and returns a *successful* response object (or `None` if the request was still not successful after `N` attempts).\n",
    "\n",
    "*Remark 1:* Your retry logic should effectively allow you to successfully request data from the site in the shortest time possible!\n",
    "\n",
    "*Remark 2:* remember your status-codes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = disc.request_until_successful('http://quotes.toscrape.com', N=1)\n",
    "isinstance(resp, requests.models.Response) or (resp is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    st = ''\n",
    "    new_endpoint = 'http://example.webscraping.com/'\n",
    "    url = new_endpoint+st\n",
    "    request = requests.get(url)\n",
    "    urlText = request.text\n",
    "    soup = bs4.BeautifulSoup(urlText, 'html.parser')\n",
    "    for i in soup.find_all('div'):\n",
    "        url = new_endpoint+st\n",
    "        request = requests.get(url)\n",
    "        urlText = urlText = request.text\n",
    "        soup = bs4.BeautifulSoup(urlText, 'html.parser')\n",
    "        att = i.attrs\n",
    "        if i.find(attrs={'id': 'pagination'})!= None:\n",
    "            check = i.find(attrs={'id': 'pagination'}).find('a').text\n",
    "            tag = i.find(attrs={'id': 'pagination'})\n",
    "            if ('Next' in check):\n",
    "                st = tag.find('a').get('href')\n",
    "                url=new_endpoint+st\n",
    "                url_list.append(url)\n",
    "            else:\n",
    "                return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Write a function `get_responses` that takes in `url_list` and returns the successful responses to each request made from the `url_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_responses(url_list):\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Write a function `parse_page` that takes in http-request response object and returns the countries listed on the page. Write a function `parse_pages` that takes in a list of response objects and returns all the countries listed on all the pages.\n",
    "\n",
    "*Remark:* Make sure to check the source page of the website in order to find appropriate tags for you to use during the scraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_page(resp):\n",
    "    \n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_pages(resps):\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Write a function `get_countries` of zero variables that puts all of this together and returns the list of countries found on all the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_countries():\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Practice Problems\n",
    "\n",
    "**Turn in the first two functions from the steps above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

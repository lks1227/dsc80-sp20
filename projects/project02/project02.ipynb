{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Project 02\n",
    "\n",
    "### Checkpoint Due Date: Thursday, January 30 11:59:59 PM (Q1-5)\n",
    "\n",
    "### Final Due Date: Thursday, February 6 11:59:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Instructions\n",
    "\n",
    "This Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems.  \n",
    "* Like the lab, your coding work will be developed in the accompanying `projectXX.py` file, that will be imported into the current notebook. This code will be autograded.\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the HW! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `projectXX.py` (much like we do in the notebook).\n",
    "- Always document your code!\n",
    "\n",
    "## Checkpoint Instructions\n",
    "\n",
    "* The checkpoint requires you to turn in **questions 1-5**; \n",
    "* The checkpoint will be graded for *approximate* correctness: easier than the final tests; harder than the doctests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T01:51:24.832772Z",
     "start_time": "2019-10-14T01:51:24.805738Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T01:51:25.681889Z",
     "start_time": "2019-10-14T01:51:24.834879Z"
    }
   },
   "outputs": [],
   "source": [
    "import project02 as proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T01:51:26.042046Z",
     "start_time": "2019-10-14T01:51:25.685618Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Investigation into Flight Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The flights dataset\n",
    "\n",
    "The department of transportation has all flight delays for listed years on their [website](https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays-on-time-data). There are data for the years 1987 - 2018. See the description of columns in `data/columns.txt`.\n",
    "\n",
    "This project will look at a single year (2015) to keep the analysis \"simple\", which is available at the URL below (*NOT* on the data.gov site).\n",
    "\n",
    "\n",
    "* To download the flights dataset to your computer, use [this link](https://dsc80-fa19-data.s3-us-west-2.amazonaws.com/project02/flight-delays.zip), unzip the file, and place `flights.csv` in your project directory.\n",
    "\n",
    "* To download the dataset on `datahub.ucsd.edu` (this works on your computer as well!):\n",
    "    - Open the terminal in datahub (\"new > Terminal\")\n",
    "    - Change the directory to where you want your data (e.g. `cd [ASSIGNMENT_PATH]/data`)\n",
    "    - Download the unzipped dataset using these commands:\n",
    "        1. `wget https://dsc80-fa19-data.s3-us-west-2.amazonaws.com/project02/flight-delays.zip`\n",
    "        2. `unzip flight-delays.zip`\n",
    "    - `flights.csv` should be in the directory.\n",
    "    \n",
    "**NOTE: The unzipped files must be in the `project02/data` directory in order for the doctests to work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your datasets\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "The flights dataset for 2015 is not small (~600MB). While you could likely load the entire dataset into Pandas on your laptop, if you wanted to work with more than one year, this would quickly become difficult (the data is available for 1987-2018). Therefore, we will filter down the dataset into two smaller files without ever reading the larger dataset fully into memory. We are going to create two smaller datasets:\n",
    "\n",
    "1. All flights arriving or departing from San Diego International Airport in 2015.\n",
    "2. All flights flown by either JetBlue or Southwest Airline in 2015.\n",
    "\n",
    "---\n",
    "\n",
    "To do this, you are going to use the `chunksize=N` keyword in Pandas `read_csv` to read the flights dataset in blocks of `N` lines. When you use this keyword argument, `pd.read_csv(fp, chunksize=N)` becomes a *iterator* that iterates through dataframes of length N until you have reached the end of the dataset. A typical pattern looks like:\n",
    "```\n",
    "L = pd.read_csv(filepath, chunksize=1000)\n",
    "for df in L:\n",
    "    process(df)\n",
    "```\n",
    "Where each `df` is a dataframe of length 1000. \n",
    "\n",
    "The processing you are going to do is:\n",
    "1. Iterate through the dataset, chunk-by-chunk,\n",
    "2. Filtering out rows of each chunk\n",
    "3. Incrementally add to a filtered csv file (since the data is perhaps too big to keep in memory). Keep in mind, if you want to keep writing to the same file, the mode='a' keyword in the `.to_csv` method can be helpful when calling it in the loop (a stands for 'append')\n",
    "\n",
    "---\n",
    "\n",
    "Write two functions that create the datasets below, using the 'chunking' pattern described above. Your functions should use `chunksize` of 10000.\n",
    "1. `get_san` which takes in a filepath containing all flights and an filepath where filtered dataset #1 is written. The function should return `None`.\n",
    "1. `get_jb_sw` which takes in a filepath containing all flights and an filepath where filtered dataset #2 is written. The function should return `None`.\n",
    "\n",
    "*Remark 1*: **Gradescope autograding servers are quite small and can't load this dataset into memory** -- so your code that reads in the large `flights.csv` dataset *must* work with chunks of the dataset one at a time to pass!\n",
    "\n",
    "*Remark 2:* You can check your work using the datasets included in the zip file!\n",
    "\n",
    "Remember to close your file properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Delays to/from San Diego\n",
    "\n",
    "The department of transportation has all flight delays for listed years on their [website](https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays-on-time-data). \n",
    "\n",
    "The zip file at the [URL](https://dsc80-fa19-data.s3-us-west-2.amazonaws.com/project02/flight-delays.zip) contains a file `to_from_san.csv` that consists of all flights either to or from SAN (San Diego) in 2015 -- i.e. the output of Question 1. This dataset should match the dataset that your code returned in question 1.\n",
    "\n",
    "Read in `to_from_san.csv` using `read_csv` and inspect the dataframe for an initial assessment about the data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T01:51:26.403770Z",
     "start_time": "2019-10-14T01:51:26.045071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "to_from_san_filepath = os.path.join('data', 'to_from_san.csv')\n",
    "flights = pd.read_csv(to_from_san_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data types of the columns\n",
    "\n",
    "**Question 2**:\n",
    "\n",
    "* First, classify the *kind* of data each column in `flights` contains. Create a function `data_kinds` of zero variables which outputs a (hard-coded) dictionary of data kinds, keyed by column name, with values `Q`, `O`, `N` (for 'Quantitative', 'Ordinal', or 'Nominal').\n",
    "\n",
    "* Second, decide the best data *type* for each column. Create a function `data_types` of zero variables which outputs a (hard-coded) dictionary of data types, keyed by column name, with values `str`, `int`, `float`, `bool`. \n",
    "\n",
    "*Remark 1*: A column which *should* be `int`s, but contains `NaN`, *must* be a float column. See Lecture 2 notes an explanation of `NaN` and data-types.\n",
    "\n",
    "*Remark 2*: As with real data, some data processing decisions may be ambiguous here. Make a best decision using the information available to you. It may be helpful to (re)read the relevant [section of the textbook](https://afraenkel.github.io/practical-data-science/03/kinds-of-data.html). \n",
    "* Certain answers *may* have more than one correct answer (in these cases, more than one choice gets full credit),\n",
    "* All answers will be graded for partial credit (some wrong answers are more wrong than other).\n",
    "There are many columns, so don't worry about the correctness of any given one; do make sure you are thinking about what's contained in a column critically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the typed flights data\n",
    "\n",
    "Read in the flights data using your dictionary of data-types in `read_csv`. This both speeds up parsing, as well as gives you the correct data-types upon reading (which columns would pandas *parse incorrectly* if you didn't use a `dtype` dictionary?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T01:51:26.410356Z",
     "start_time": "2019-10-14T01:51:24.819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "dtypes = proj.data_types()\n",
    "flights = pd.read_csv(to_from_san_filepath, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (Basic Stats):**\n",
    "\n",
    "Define a function `basic_stats` that takes a dataframe `flights` and outputs a dataframe that contains statistics for flights arriving/departing for SAN. That is, the output should have two rows, indexed by `ARRIVING` and `DEPARTING`, and have the following columns:\n",
    "\n",
    "1. number of arriving/departing flights to/from SAN (`count`).\n",
    "2. mean flight (arrival) delay of arriving/departing flights to/from SAN (`mean_delay`).\n",
    "3. median flight (arrival) delay of arriving/departing flights to/from SAN (`median_delay`).\n",
    "4. the airline code of the airline with the longest flight (arrival) delay among all flights arriving/departing to/from SAN (`airline`).\n",
    "5. a list of the three months with the greatest number of arriving/departing flights to/from SAN, sorted from greatest to least (`top_months`).\n",
    "\n",
    "*Remark:* Null values should not be considered when computing statistics; however, think about whether e.g. the average flight delay is likely higher or lower that the \"true mean\" by making this choice.\n",
    "\n",
    "*Hint*: Use `groupbby` and the fact that `aggregate` can take in a dictionary as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(index = ['ARRIVING','DEPARTING'])\n",
    "From_SAN = len(flights[flights['ORIGIN_AIRPORT']=='SAN'])\n",
    "To_SAN = len(flights[flights['DESTINATION_AIRPORT']=='SAN'])\n",
    "result['count'] = [To_SAN,From_SAN]\n",
    "From_san_delay = flights[flights['ORIGIN_AIRPORT']=='SAN']['ARRIVAL_DELAY'].mean()\n",
    "To_san_delay = flights[flights['DESTINATION_AIRPORT']=='SAN']['ARRIVAL_DELAY'].mean()\n",
    "result['mean_delay']=[To_san_delay,From_san_delay]\n",
    "From_san_delays = flights[flights['ORIGIN_AIRPORT']=='SAN']['ARRIVAL_DELAY'].median()\n",
    "To_san_delays = flights[flights['DESTINATION_AIRPORT']=='SAN']['ARRIVAL_DELAY'].median()\n",
    "result['median_delay']=[To_san_delays,From_san_delays]\n",
    "From_max = flights[flights['ORIGIN_AIRPORT']=='SAN']['ARRIVAL_DELAY'].max()\n",
    "airline_from = flights[flights['ARRIVAL_DELAY']==From_max]['AIRLINE'].values[0]\n",
    "To_max = flights[flights['DESTINATION_AIRPORT']=='SAN']['ARRIVAL_DELAY'].max()\n",
    "airline_to = flights[flights['ARRIVAL_DELAY']==From_max]['AIRLINE'].values[0]\n",
    "result['airline']=[airline_to,airline_from]\n",
    "lis_to = list(flights[flights['DESTINATION_AIRPORT']=='SAN']['MONTH'].value_counts()[:3].index.values)\n",
    "lis_from = list(flights[flights['ORIGIN_AIRPORT']=='SAN']['MONTH'].value_counts()[:3].index.values)\n",
    "result['top_months']=[lis_to,lis_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding flight delays: Departures, Arrivals, and everything in-between\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "Often `DEPARTURE_DELAY` is thought to be the main cause of a flight delay -- i.e., when the flight is late pushing off from the gate. \n",
    "\n",
    "However, there are other ways that flights can be late: waiting on the tarmac, headwinds, turbulence, circling a busy airport, and waiting for a gate after landing. First, we will analyze all the ways in which a flight can be delayed.\n",
    "\n",
    "* First, create a function `depart_arrive_stats` that takes in a dataframe like `flights` and calculates the following quantities in a series, indexed by `late1`, `late2`, `late3`:\n",
    "    - The proportion of flights from/to SAN that leave late, but arrive early or on-time (`late1`).\n",
    "    - The proportion of flights from/to SAN that leaves early, or on-time, but arrives late (`late2`).\n",
    "    - The proportion of flights from/to SAN that both left late and arrived late (`late3`).\n",
    "    \n",
    "* Second, create a function `depart_arrive_stats_by_month` that takes in a dataframe like `flights` and calculates the quantities above broken down by *month*. That is, the output is a dataframe, indexed by `MONTH`, with columns given by `late1`, `late2`, `late3`.\n",
    "\n",
    "*Remark 1:* Does this question reveal any data quality issues? Can you pinpoint when these issues occur?\n",
    "\n",
    "*Remark 2:* A flight is considered late if it departed/arrived any time later than its planned departure/arrival time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv(to_from_san_filepath)\n",
    "flights = flights.fillna(0)\n",
    "flights['dep_late']=flights['DEPARTURE_DELAY']>0\n",
    "flights['arr_late']=flights['ARRIVAL_DELAY']>0\n",
    "temp1 = np.array(flights[flights['ORIGIN_AIRPORT']=='SAN'].index.to_list())\n",
    "temp2 = np.array(flights[flights['DESTINATION_AIRPORT']=='SAN'].index.to_list())\n",
    "indexs = np.union1d(temp1,temp2)\n",
    "flights_san = flights.loc[ indexs , : ]\n",
    "temp1 = np.array(flights[flights['DEPARTURE_DELAY']>0].index.to_list())\n",
    "temp2 = np.array(flights[flights['ARRIVAL_DELAY']<=0].index.to_list())\n",
    "indexs = np.intersect1d(temp1,temp2)\n",
    "flights_late1 = flights.loc[ indexs , : ]\n",
    "late1 = len(flights_late1)/len(flights_san)\n",
    "temp1 = np.array(flights[flights['DEPARTURE_DELAY']<=0].index.to_list())\n",
    "temp2 = np.array(flights[flights['ARRIVAL_DELAY']>0].index.to_list())\n",
    "indexs = np.intersect1d(temp1,temp2)\n",
    "flights_late2 = flights.loc[ indexs , : ]\n",
    "late2 = len(flights_late2)/len(flights_san)\n",
    "temp1 = np.array(flights[flights['DEPARTURE_DELAY']>0].index.to_list())\n",
    "temp2 = np.array(flights[flights['ARRIVAL_DELAY']>0].index.to_list())\n",
    "indexs = np.intersect1d(temp1,temp2)\n",
    "flights_late3 = flights.loc[ indexs , : ]\n",
    "late3 = len(flights_late3)/len(flights_san)\n",
    "result = pd.Series([late1,late2,late3],index = ['late1','late2','late3'] )\n",
    "result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv(to_from_san_filepath)\n",
    "flights = flights.fillna(0)\n",
    "flights['dep_late']=flights['DEPARTURE_DELAY']>0\n",
    "flights['arr_late']=flights['ARRIVAL_DELAY']>0\n",
    "month = flights['MONTH'].unique()\n",
    "lis1=flights['dep_late'].to_list()\n",
    "lis2=flights['arr_late'].to_list()\n",
    "flights['late1'] = pd.Series(np.array(lis1) & ~np.array(lis2))\n",
    "flights['late2']=pd.Series(~np.array(lis1) & np.array(lis2)) \n",
    "flights['late3']=pd.Series(np.array(lis2) & np.array(lis1) )\n",
    "data = flights.groupby(['MONTH','late1'],as_index=False).count()\n",
    "total = flights.groupby(['MONTH']).count()['YEAR']\n",
    "la1 = data[data['late1']==True]['YEAR'].to_list()\n",
    "lat1 = pd.Series(la1,index=month)\n",
    "late1 = lat1/total\n",
    "data = flights.groupby(['MONTH','late2'],as_index=False).count()\n",
    "la2 = data[data['late2']==True]['YEAR'].to_list()\n",
    "lat2 = pd.Series(la2,index=month)\n",
    "late2 = lat2/total\n",
    "data = flights.groupby(['MONTH','late3'],as_index=False).count()\n",
    "la3 = data[data['late3']==True]['YEAR'].to_list()\n",
    "lat3 = pd.Series(la3,index=month)\n",
    "late3 = lat3/total\n",
    "frame = { 'late1': late1, 'late2': late2,'late3':late3 } \n",
    "  \n",
    "result = pd.DataFrame(frame)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj.depart_arrive_stats_by_month(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flights['late1'] = pd.Series(np.array(lis1) & ~np.array(lis2))\n",
    "b = flights[flights['late1']==True].index.values\n",
    "c = np.setdiff1d(np.union1d(a, b), np.intersect1d(a, b))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight delays and day of the week\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "Next, we'd like to understand the flight traffic to/from SAN by day of the week. Day of the week is specified by integers 1 through 7; verify for yourself which integer corresponds to which day (hint: you have the *date* for each flight as well!).\n",
    "\n",
    "Next create two functions to understand both the amount of traffic and the average flight delay of flights for each airline by day-of-the week. We both want to understand *presence* each airline has as well as their *performance*.\n",
    "\n",
    "1. Create a function `cnts_by_airline_dow` that takes in a dataframe like `flights` and outputs a dataframe that answers the following question: Given any `AIRLINE` and `DAY_OF_WEEK`, how many flights were there (in 2015)?\n",
    "\n",
    "\n",
    "2. Create a function `mean_by_airline_dow` that takes in a dataframe like `flights` and outputs a dataframe that answers the following question: Given any `AIRLINE` and `DAY_OF_WEEK`, what is the average `ARRIVAL_DELAY` (in 2015)?\n",
    "\n",
    "Both dataframes should have a column for each distinct value of `AIRLINE` and a row for each `DAY_OF_WEEK`.\n",
    "\n",
    "*Hint:* Both `groupby` and `pivot` should be useful here!\n",
    "\n",
    "Your output should have the *form* of the table below (not the entries themselves!)\n",
    "\n",
    "<img src=\"pivot.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = flights.groupby('AIRLINE').count().index.values\n",
    "flights.groupby(['DAY_OF_WEEK','AIRLINE'],as_index=False)['ARRIVAL_DELAY'].mean().pivot('DAY_OF_WEEK','AIRLINE', 'ARRIVAL_DELAY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding null values in the flights data\n",
    "\n",
    "**Question 6 (Missing by Design)**\n",
    "\n",
    "Now we would like to understand how data is missing in the flights data. First, compute the proportion of each column of `flights` which are non-null. Do not turn this in, but it will be useful information in doing the next few problems.\n",
    "\n",
    "Recall that a column is *missing by design* if you can determine when the entry of a column is missing based solely on other data in the same row. That is\n",
    "* there is *no randomness* in determining when an entry is missing.\n",
    "* you can describe when the column is missing a value with a logical (not random) condition.\n",
    "* you can express which rows will have missing values in terms of logical statements about the *other* columns in the same row.\n",
    "\n",
    "For this question, verify the following columns are *missing by design*:\n",
    "* The column `ARRIVAL_DELAY` is *missing by design*. Create a function `predict_null_arrival_delay` that doesn't depend on the values of `ARRIVAL_DELAY`, that:\n",
    "    - Takes in a row of the flights data (that is, a Series)\n",
    "    - Returns `True` if and only if the `ARRIVAL_DELAY` is null; otherwise it returns `False`.\n",
    "    - Since the function doesn't depend on `ARRIVAL_DELAY`, it should work on a row even if the `ARRIVAL_DELAY` index is dropped.\n",
    "    - You can check your function by using `flights.drop('ARRIVAL_DELAY', axis=1).apply(predict_null, axis=1)` and compare it to the `ARRIVAL_DELAY` column!\n",
    "\n",
    "\n",
    "* The column `AIRLINE_DELAY` is *missing by design*. As above, create a function `predict_null_airline_delay` that doesn't depend on the values of `AIRLINE_DELAY`, that:\n",
    "    - Takes in a row of the flights data (that is, a Series)\n",
    "    - Returns `True` if and only if the `AIRLINE_DELAY` is null; otherwise it returns `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_from_san_filepath = os.path.join('data', 'to_from_san.csv')\n",
    "flights = pd.read_csv(to_from_san_filepath)\n",
    "pd.set_option(\"display.max_column\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     138564\n",
       "False      1850\n",
       "Name: ARRIVAL_DELAY, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = flights.drop('ARRIVAL_DELAY', axis=1).apply(proj.predict_null_arrival_delay, axis=1)\n",
    "(flights['ARRIVAL_DELAY']==flights['ARRIVAL_DELAY']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis = flights[flights['AIRLINE_DELAY']==flights['AIRLINE_DELAY']].index == flights[flights['SECURITY_DELAY']==flights['SECURITY_DELAY']].index\n",
    "False in lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    115157\n",
       "Name: WEATHER_DELAY, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = flights[flights['AIRLINE_DELAY']==flights['AIRLINE_DELAY']]\n",
    "tables = flights[flights['AIRLINE_DELAY']!=flights['AIRLINE_DELAY']]\n",
    "tables['WEATHER_DELAY'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (Missingness Types)**\n",
    "\n",
    "Now we'd like to determine missingness of the column `DEPARTURE_DELAY`. In particular, we'd like to perform a permutation test to determine the missingness of `DEPARTURE_DELAY` dependent on the column `AIRLINE`.\n",
    "\n",
    "* Create a function `perm4missing`:\n",
    "    - that takes in `flights`, a column `col`, and a number `N` and \n",
    "    - returns the p-value of the test (using `N` simulations) that determines if `DEPARTURE_DELAY` is MAR dependent on `col`. That is `perm4missing(flights, 'AIRLINE', N)` should return the p-value for the test above.\n",
    "    - *Remark*: to help your work, create helper functions whose output you can plot, to assess the correctness of your p-value!\n",
    "    \n",
    "* Use your function above to determine the columns `col` for which \"`DEPARTURE_DELAY` is MAR dependent on `col`\" using a significance level of 0.01. Only consider the categorical columns `YEAR`,`DAY_OF_WEEK`, `AIRLINE`,`DIVERTED`, `CANCELLATION_REASON`. Return your answer in a (hard-coded) list returned by a function called `dependent_cols`.\n",
    "\n",
    "* Create a function `missing_types` of zero variables, which:\n",
    "    - Returns a Series, indexed by the following columns of `flights`: `CANCELLED`, `CANCELLATION_REASON`, `TAIL_NUMBER`, `ARRIVAL_TIME`.\n",
    "    - The values should contain the most-likely missingness type of each column. \n",
    "    - The values of this Series should be `MD, MCAR, MAR, MNAR, NaN` (use `NaN` if there are no missing values). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8d076fe77eb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflight_mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'AIRLINE'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m distr = (\n\u001b[0;32m      4\u001b[0m     \u001b[0mflight_mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_null\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflight_mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DEPARTURE_DELAY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flights' is not defined"
     ]
    }
   ],
   "source": [
    "flight_mar = flights.copy()\n",
    "col = 'AIRLINE'\n",
    "distr = (\n",
    "    flight_mar\n",
    "    .assign(is_null=flight_mar['DEPARTURE_DELAY'].isnull())\n",
    "    .pivot_table(index='is_null', columns=col, aggfunc='size',fill_value = 0)\n",
    "    .apply(lambda x:x / x.sum(), axis=1)\n",
    ")\n",
    "obs = distr.diff().iloc[-1].abs().sum() / 2\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02737152690033778,\n",
       " 0.02843236595124949,\n",
       " 0.037989037661723055,\n",
       " 0.04426304365157686,\n",
       " 0.03451862796154574,\n",
       " 0.025905256769942757,\n",
       " 0.031611860099019305,\n",
       " 0.02899831347065359,\n",
       " 0.02975737464623157,\n",
       " 0.02078667402025433,\n",
       " 0.039103112342447635,\n",
       " 0.022748696470011476,\n",
       " 0.028007936054119782,\n",
       " 0.03630166802240752,\n",
       " 0.02615647360629736,\n",
       " 0.04414571006902493,\n",
       " 0.04193124108597817,\n",
       " 0.02915693412779953,\n",
       " 0.026702008156579783,\n",
       " 0.02502282624935674,\n",
       " 0.019516950449976866,\n",
       " 0.019356208565617938,\n",
       " 0.031168503116564418,\n",
       " 0.022908352122077754,\n",
       " 0.0338886747166727,\n",
       " 0.03239284062246438,\n",
       " 0.024509017879998415,\n",
       " 0.02233486246147208,\n",
       " 0.02847909443478106,\n",
       " 0.01862827921744548,\n",
       " 0.023443751920120527,\n",
       " 0.02287328526448012,\n",
       " 0.033774128447173535,\n",
       " 0.02407428927103771,\n",
       " 0.018932875148255344,\n",
       " 0.02864503178869051,\n",
       " 0.03525970481944872,\n",
       " 0.03279487978790189,\n",
       " 0.02525537218724757,\n",
       " 0.0216810530960592,\n",
       " 0.05742788186137588,\n",
       " 0.036714508025924464,\n",
       " 0.028748326331234038,\n",
       " 0.032688767189882215,\n",
       " 0.024865148359860985,\n",
       " 0.024623021033351206,\n",
       " 0.025767346258677833,\n",
       " 0.023896782518464423,\n",
       " 0.029902714576479056,\n",
       " 0.037237344420280885,\n",
       " 0.03956536566780436,\n",
       " 0.024200599641215445,\n",
       " 0.03816345992448434,\n",
       " 0.0323602946435836,\n",
       " 0.02291318893002225,\n",
       " 0.03331657871259341,\n",
       " 0.02724438648472889,\n",
       " 0.01805387753161294,\n",
       " 0.027134799992867747,\n",
       " 0.03990794898643256,\n",
       " 0.02715707800234094,\n",
       " 0.022453497473280227,\n",
       " 0.03044868976816318,\n",
       " 0.0317360082520862,\n",
       " 0.03395507835116496,\n",
       " 0.04510436130463466,\n",
       " 0.023698975518988474,\n",
       " 0.033955877654172746,\n",
       " 0.03004644565323654,\n",
       " 0.02611888586998292,\n",
       " 0.038486880465874784,\n",
       " 0.025285776443966484,\n",
       " 0.02866667445474723,\n",
       " 0.03157025535271703,\n",
       " 0.028704446645601955,\n",
       " 0.020755900854454924,\n",
       " 0.01923405867007044,\n",
       " 0.03130392349153589,\n",
       " 0.03799115888893602,\n",
       " 0.02525695029831425,\n",
       " 0.02676225305892236,\n",
       " 0.01948377937515408,\n",
       " 0.025187687618448,\n",
       " 0.013828813069874862,\n",
       " 0.04338923125701055,\n",
       " 0.031061734680179386,\n",
       " 0.027975082651005238,\n",
       " 0.025956063748308916,\n",
       " 0.035517859196011824,\n",
       " 0.019533899772731518,\n",
       " 0.01747152380560052,\n",
       " 0.030476819085552276,\n",
       " 0.024908495176821192,\n",
       " 0.025126827867637876,\n",
       " 0.02596638295508883,\n",
       " 0.027884925370717746,\n",
       " 0.033061447340995545,\n",
       " 0.02451364973845369,\n",
       " 0.03486547422954876,\n",
       " 0.029101669498043853,\n",
       " 0.028778669103106183,\n",
       " 0.022819352806404087,\n",
       " 0.0251446789681449,\n",
       " 0.028108822440165528,\n",
       " 0.020697920643967714,\n",
       " 0.028699169196255724,\n",
       " 0.020492981402268524,\n",
       " 0.033915338645214184,\n",
       " 0.01991249271660754,\n",
       " 0.026735681357651024,\n",
       " 0.03128817312329287,\n",
       " 0.029226883388454486,\n",
       " 0.025545795860881078,\n",
       " 0.022336532799808823,\n",
       " 0.0339229012813647,\n",
       " 0.025658917731430506,\n",
       " 0.024607342397429434,\n",
       " 0.032936715081884504,\n",
       " 0.027549628006429662,\n",
       " 0.030267012293485204,\n",
       " 0.021440524375565005,\n",
       " 0.03314347837403732,\n",
       " 0.02980225858436058,\n",
       " 0.03518280777111079,\n",
       " 0.01994749808935841,\n",
       " 0.04936054734630377,\n",
       " 0.03576017097706187,\n",
       " 0.035113914000325055,\n",
       " 0.05214595436881603,\n",
       " 0.0372294128750499,\n",
       " 0.026732555877941114,\n",
       " 0.028338212155923113,\n",
       " 0.03347415412734451,\n",
       " 0.04333046199099001,\n",
       " 0.020927976445565134,\n",
       " 0.024721263570986612,\n",
       " 0.019236804993225387,\n",
       " 0.02561611403061656,\n",
       " 0.02480285396262666,\n",
       " 0.03272614997670747,\n",
       " 0.023154055817173558,\n",
       " 0.030209267774910523,\n",
       " 0.0349227576117728,\n",
       " 0.03353983019115023,\n",
       " 0.03356836940751769,\n",
       " 0.019330138990595082,\n",
       " 0.0236016757490033,\n",
       " 0.02285502426499477,\n",
       " 0.031180923055608323,\n",
       " 0.028399717497624127,\n",
       " 0.039852090003158265,\n",
       " 0.032549975395813835,\n",
       " 0.028604154613074816,\n",
       " 0.02292271908126884,\n",
       " 0.020359118643414737,\n",
       " 0.019473818829980245,\n",
       " 0.02398896879869472,\n",
       " 0.03868668572287016,\n",
       " 0.03213986122050293,\n",
       " 0.034389243349030416,\n",
       " 0.0345595358795848,\n",
       " 0.01877332197093365,\n",
       " 0.03244062459586521,\n",
       " 0.03541974987554443,\n",
       " 0.024305062395847246,\n",
       " 0.028681174631106296,\n",
       " 0.03354355002437873,\n",
       " 0.02695637096759318,\n",
       " 0.025720166886270046,\n",
       " 0.031575573791961103,\n",
       " 0.028844632096196733,\n",
       " 0.019870693268290586,\n",
       " 0.035427681420775445,\n",
       " 0.033491021470303535,\n",
       " 0.025555346507076613,\n",
       " 0.025772510985805006,\n",
       " 0.039671478265823984,\n",
       " 0.03787420446293907,\n",
       " 0.023319921438761822,\n",
       " 0.018622663601442088,\n",
       " 0.02269975453199681,\n",
       " 0.019502593738260243,\n",
       " 0.03782664593397635,\n",
       " 0.03132210251122558,\n",
       " 0.028628779294199047,\n",
       " 0.038536375767510234,\n",
       " 0.020547928360315944,\n",
       " 0.027289854528902074,\n",
       " 0.0407111152920807,\n",
       " 0.03403666874280501,\n",
       " 0.023724799154624332,\n",
       " 0.026661110486015174,\n",
       " 0.02281308135203539,\n",
       " 0.020310945265984467,\n",
       " 0.03411133184171095,\n",
       " 0.04168835544635848,\n",
       " 0.024523528303831872,\n",
       " 0.04156150196003443,\n",
       " 0.029441424513721448,\n",
       " 0.035970233955990374,\n",
       " 0.031706782454930076,\n",
       " 0.029442787427824448,\n",
       " 0.020963391717294305,\n",
       " 0.03865918150142306,\n",
       " 0.023732044119066657,\n",
       " 0.022069914009342842,\n",
       " 0.029538918985721384,\n",
       " 0.03239186711239083,\n",
       " 0.01649461157050541,\n",
       " 0.028824014177585865,\n",
       " 0.039217699601844624,\n",
       " 0.03375302889726315,\n",
       " 0.03983492548343999,\n",
       " 0.018588723966034972,\n",
       " 0.02578445954102381,\n",
       " 0.027944104535716642,\n",
       " 0.034900254157861535,\n",
       " 0.02193585654847434,\n",
       " 0.02473107040404357,\n",
       " 0.023893482831688725,\n",
       " 0.029447173346892785,\n",
       " 0.029227795413681323,\n",
       " 0.021113547960537375,\n",
       " 0.01826411471760725,\n",
       " 0.04088985174158903,\n",
       " 0.01294922109971386,\n",
       " 0.027378905081948034,\n",
       " 0.0333065566825728,\n",
       " 0.03116315393489697,\n",
       " 0.022453569205601424,\n",
       " 0.021988825743951272,\n",
       " 0.026197924640482755,\n",
       " 0.016359211690482815,\n",
       " 0.033170910863163205,\n",
       " 0.02463766467435267,\n",
       " 0.03894706380139083,\n",
       " 0.026659470890101757,\n",
       " 0.023485787060350014,\n",
       " 0.026415058376787745,\n",
       " 0.04239120922453056,\n",
       " 0.027635676049459233,\n",
       " 0.02153072264575033,\n",
       " 0.02051571030061787,\n",
       " 0.0272274269144998,\n",
       " 0.029376998641799726,\n",
       " 0.04198737675106283,\n",
       " 0.036659356118387845,\n",
       " 0.025891217729934364,\n",
       " 0.022559138687474785,\n",
       " 0.02190441729683511,\n",
       " 0.019721787216931422,\n",
       " 0.032311024786386255,\n",
       " 0.030507602498826142,\n",
       " 0.021118528233124302,\n",
       " 0.02554498631039885,\n",
       " 0.025546820608326977,\n",
       " 0.024678890264099952,\n",
       " 0.024989880618972046,\n",
       " 0.021171528171024606,\n",
       " 0.03076626924911223,\n",
       " 0.036843482739461454,\n",
       " 0.02894405309339486,\n",
       " 0.03364885307191615,\n",
       " 0.017588775408356717,\n",
       " 0.02757048161695307,\n",
       " 0.02556039851198474,\n",
       " 0.03620381488880159,\n",
       " 0.035464131687424974,\n",
       " 0.026567499806835114,\n",
       " 0.02999114003358303,\n",
       " 0.02111878441998579,\n",
       " 0.02980157200357187,\n",
       " 0.03151002069784891,\n",
       " 0.0336897200000574,\n",
       " 0.017206237186814122,\n",
       " 0.026732555877941107,\n",
       " 0.051527611512504665,\n",
       " 0.030932032395955843,\n",
       " 0.02360600018322486,\n",
       " 0.02338624309345838,\n",
       " 0.02850531772192085,\n",
       " 0.015281659008729012,\n",
       " 0.026115586183207203,\n",
       " 0.029994542195103318,\n",
       " 0.02249899625987678,\n",
       " 0.03989910541597469,\n",
       " 0.04442766932875556,\n",
       " 0.044810627696751,\n",
       " 0.023845258216886156,\n",
       " 0.04177511056512567,\n",
       " 0.021745458473032053,\n",
       " 0.01816685593751993,\n",
       " 0.019481637652992254,\n",
       " 0.030417281258947333,\n",
       " 0.050111799946344224,\n",
       " 0.028978392380305917,\n",
       " 0.041274726387308225,\n",
       " 0.023075099026469433,\n",
       " 0.03716414671022252,\n",
       " 0.030489976842757213,\n",
       " 0.029330721047144313,\n",
       " 0.02134806141352434,\n",
       " 0.0304074846733648,\n",
       " 0.02950332950692637,\n",
       " 0.031291073158564726,\n",
       " 0.028050217133736316,\n",
       " 0.02148334857132786,\n",
       " 0.031157774010806175,\n",
       " 0.023124256161447712,\n",
       " 0.017540417576386202,\n",
       " 0.020508875235153906,\n",
       " 0.025520515341391544,\n",
       " 0.022824128129501846,\n",
       " 0.026706906449371022,\n",
       " 0.020208532006244416,\n",
       " 0.025209453254198242,\n",
       " 0.018629088767927703,\n",
       " 0.03077285837518915,\n",
       " 0.02434309077356342,\n",
       " 0.02531675455925509,\n",
       " 0.022517790128033996,\n",
       " 0.03167149015289437,\n",
       " 0.02967899171409709,\n",
       " 0.02811276771783211,\n",
       " 0.036467287704608775,\n",
       " 0.017305658184012248,\n",
       " 0.023573095542738003,\n",
       " 0.022408572545253382,\n",
       " 0.03787744266486802,\n",
       " 0.016082888541704456,\n",
       " 0.04457546865287316,\n",
       " 0.041776668181243384,\n",
       " 0.03464443620547498,\n",
       " 0.026496372086617384,\n",
       " 0.04357234337811275,\n",
       " 0.03942044588400964,\n",
       " 0.029944206600562247,\n",
       " 0.03089435243237127,\n",
       " 0.028887630499025374,\n",
       " 0.01613530437356061,\n",
       " 0.035038564320630385,\n",
       " 0.028315022121223103,\n",
       " 0.02712641755876054,\n",
       " 0.03229306096366019,\n",
       " 0.024401234943641957,\n",
       " 0.03531989848441903,\n",
       " 0.04890315132384049,\n",
       " 0.033369127761617504,\n",
       " 0.022690552299932908,\n",
       " 0.020195148804601347,\n",
       " 0.021718784297016076,\n",
       " 0.028859716378599912,\n",
       " 0.041673629825561345,\n",
       " 0.043079142679890825,\n",
       " 0.026698052631438724,\n",
       " 0.022284168205324972,\n",
       " 0.03599346498058817,\n",
       " 0.03783264070653468,\n",
       " 0.02762658653961439,\n",
       " 0.038267236098327405,\n",
       " 0.02971077937986792,\n",
       " 0.032660248468463694,\n",
       " 0.025355561745030116,\n",
       " 0.028714642882688365,\n",
       " 0.026537382479401062,\n",
       " 0.03501750576061777,\n",
       " 0.02359141802707012,\n",
       " 0.048533791354451745,\n",
       " 0.027256878156094025,\n",
       " 0.02740497465697091,\n",
       " 0.03320506569553398,\n",
       " 0.03627838576043743,\n",
       " 0.03449745667931407,\n",
       " 0.020802916267271345,\n",
       " 0.03653575108146721,\n",
       " 0.024436025119429224,\n",
       " 0.02089112652741167,\n",
       " 0.030597493344777706,\n",
       " 0.036845511739404264,\n",
       " 0.02735861508251985,\n",
       " 0.026714212898660068,\n",
       " 0.03866878338499085,\n",
       " 0.03530282619197087,\n",
       " 0.018166025892088766,\n",
       " 0.025751882819719664,\n",
       " 0.02323296137050542,\n",
       " 0.03298541108051216,\n",
       " 0.04049297730327873,\n",
       " 0.03677870845340809,\n",
       " 0.02380887968255784,\n",
       " 0.03674773033811951,\n",
       " 0.019910842873219675,\n",
       " 0.01962786911351761,\n",
       " 0.021699344837967965,\n",
       " 0.020846929170071388,\n",
       " 0.025980237540556943,\n",
       " 0.022906681783740966,\n",
       " 0.025414187546408257,\n",
       " 0.02678156954827697,\n",
       " 0.03041414553176298,\n",
       " 0.03892467306969863,\n",
       " 0.042947349910877716,\n",
       " 0.022771538090579878,\n",
       " 0.033638072728785606,\n",
       " 0.01852572249306297,\n",
       " 0.03511609671238476,\n",
       " 0.029493840345577626,\n",
       " 0.027602105323132585,\n",
       " 0.03202192303695792,\n",
       " 0.021058713724708973,\n",
       " 0.034793004090176965,\n",
       " 0.022055670019845264,\n",
       " 0.02771770708250096,\n",
       " 0.018705873094046616,\n",
       " 0.02006776244960548,\n",
       " 0.01976350468545275,\n",
       " 0.03107875573525528,\n",
       " 0.024293452007285526,\n",
       " 0.02893642897239761,\n",
       " 0.03782226001490802,\n",
       " 0.021832090622105726,\n",
       " 0.021261634213939812,\n",
       " 0.028709795827269385,\n",
       " 0.04211508077776693,\n",
       " 0.01810033958080863,\n",
       " 0.02958640578236292,\n",
       " 0.0343282606285268,\n",
       " 0.031665720824774125,\n",
       " 0.03348173725844397,\n",
       " 0.04438920030963768,\n",
       " 0.02441828674114121,\n",
       " 0.03367502512168366,\n",
       " 0.039116608266309735,\n",
       " 0.026109458193480935,\n",
       " 0.02361771304653113,\n",
       " 0.028441281254028537,\n",
       " 0.035813252894757806,\n",
       " 0.040909373180432806,\n",
       " 0.03027763892449886,\n",
       " 0.022940211520169777,\n",
       " 0.03333808816148215,\n",
       " 0.04457291703173295,\n",
       " 0.03351484684841995,\n",
       " 0.01933049765220113,\n",
       " 0.021737178513669415,\n",
       " 0.03521111129556566,\n",
       " 0.034490283447193036,\n",
       " 0.034580666171918614,\n",
       " 0.03903757974328437,\n",
       " 0.032494598043839085,\n",
       " 0.03649204560290094,\n",
       " 0.03830859490524262,\n",
       " 0.026892170540109544,\n",
       " 0.022806225791622524,\n",
       " 0.042464089262880735,\n",
       " 0.019876042449957984,\n",
       " 0.018671462074814356,\n",
       " 0.025540210987301123,\n",
       " 0.030861437544409988,\n",
       " 0.02706806843919282,\n",
       " 0.021931870280909906,\n",
       " 0.031760243529181005,\n",
       " 0.02292147913685933,\n",
       " 0.03196028447808894,\n",
       " 0.02965106734619719,\n",
       " 0.023115986449559575,\n",
       " 0.041719497521238394,\n",
       " 0.030421052329548143,\n",
       " 0.030781107592128376,\n",
       " 0.02554094880546216,\n",
       " 0.03284424187236935,\n",
       " 0.03110535817895,\n",
       " 0.015802743084953,\n",
       " 0.03519862987167496,\n",
       " 0.025638730206746896,\n",
       " 0.024263160472785695,\n",
       " 0.024789788432691834,\n",
       " 0.01847041687340948,\n",
       " 0.023914162235146363,\n",
       " 0.03102795900436357,\n",
       " 0.018829847040047747,\n",
       " 0.02358429603232132,\n",
       " 0.0246081724428606,\n",
       " 0.02092702343044045,\n",
       " 0.025589501339447387,\n",
       " 0.034653556457743206,\n",
       " 0.018654594731855358,\n",
       " 0.010752674949495312,\n",
       " 0.028291934561267718,\n",
       " 0.028962601022165073,\n",
       " 0.0151244934929562,\n",
       " 0.032991098428836726,\n",
       " 0.02595707824828032,\n",
       " 0.026810385446454782,\n",
       " 0.031996109648796484,\n",
       " 0.020119409720877224,\n",
       " 0.03389387018622325,\n",
       " 0.03196293857397374,\n",
       " 0.026670148758487746,\n",
       " 0.02240096891920502]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_repetitions = 500\n",
    "col = 'AIRLINE'\n",
    "tvds = []\n",
    "for i in range(n_repetitions):\n",
    "    print(i)\n",
    "    # shuffle the gender column\n",
    "    shuffled_col = (\n",
    "        flight_mar[col]\n",
    "        .sample(replace=False, frac=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # put them in a table\n",
    "    shuffled = (\n",
    "        flight_mar\n",
    "        .assign(**{\n",
    "            col: shuffled_col,\n",
    "            'is_null': flight_mar['DEPARTURE_DELAY'].isnull()\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # compute the tvd\n",
    "    shuffled = (\n",
    "        shuffled\n",
    "        .pivot_table(index='is_null', columns=col, aggfunc='size',fill_value=0)\n",
    "        .apply(lambda x:x / x.sum(), axis=1)\n",
    "    )\n",
    "    \n",
    "    tvd = shuffled.diff().iloc[-1].abs().sum() / 2\n",
    "    # add it to the list of results\n",
    "    \n",
    "    tvds.append(tvd)\n",
    "tvds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "      ...  \n",
       "95    False\n",
       "96    False\n",
       "97    False\n",
       "98    False\n",
       "99     True\n",
       "Length: 100, dtype: bool"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'to_from_san.csv')\n",
    "flights = pd.read_csv(fp, nrows=100)\n",
    "out = flights.drop('AIRLINE_DELAY', axis=1).apply(proj.predict_null_airline_delay, axis=1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = (\n",
    "    heights_mar\n",
    "    .assign(is_null=heights_mar.child.isnull())\n",
    "    .pivot_table(index='is_null', columns='gender', aggfunc='size')\n",
    "    .apply(lambda x:x / x.sum(), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.03, 0.912]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[proj.perm4missing(flights, 'YEAR', 500),proj.perm4missing(flights, 'DAY_OF_WEEK', 500),proj.perm4missing(flights, 'AIRLINE', 500),proj.perm4missing(flights, 'DIVERTED', 500),proj.perm4missing(flights, 'CANCELLATION_REASON', 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR,DAY_OF_WEEK, AIRLINE,DIVERTED, CANCELLATION_REASON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpson's Paradox: JetBlue vs Southwest\n",
    "\n",
    "The remainder of the questions investigates the presence of Simpson's paradox in the flights dataset. Read through the final slides of lecture 05, as well as [the book](https://afraenkel.github.io/practical-data-science/05/understanding-aggregations.html#simpsons-paradox) for a summary of Simpson's Paradox and related links.\n",
    "\n",
    "The csv file `southwest_vs_jetblue.csv` contains all Southwest and JetBlue flights in 2015.\n",
    "\n",
    "In this dataset, we are going to verify the following occurrences of Simpson's Paradox: For a given set of airports,\n",
    "* The average departure delay of Southwest is greater than (or less than) the average departure delay of JetBlue.\n",
    "* Airport by airport, the average departure delay of Southwest is *less* than (or greater than) the average departure delay of JetBlue.\n",
    "\n",
    "That is, the inequalities of the average flight delays between the two airlines are reversed when viewed at the level of each airport. In fact this reversal holds for *every* airport being considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T01:51:26.412337Z",
     "start_time": "2019-10-14T01:51:24.838Z"
    }
   },
   "outputs": [],
   "source": [
    "jb_sw_filepath = os.path.join('data', 'jetblue_or_sw.csv')\n",
    "dtype = proj.data_types()\n",
    "\n",
    "# The `usecols` keyword:\n",
    "# choose *only* the columns you need to reduce your memory footprint!\n",
    "usecols = ...\n",
    "\n",
    "jb_sw = pd.read_csv(jb_sw_filepath, dtype=dtype, usecols=usecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-6eeded991b34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mproj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperm4missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CANCELLATION_REASON'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\DSC30-pa10\\src\\dsc80-sp20\\projects\\project02\\project02.py\u001b[0m in \u001b[0;36mperm4missing\u001b[1;34m(flights, col, N)\u001b[0m\n\u001b[0;32m    362\u001b[0m             .assign(**{\n\u001b[0;32m    363\u001b[0m                 \u001b[0mcol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mshuffled_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m                 \u001b[1;34m'is_null'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mflight_mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DEPARTURE_DELAY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             })\n\u001b[0;32m    366\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   3562\u001b[0m         \u001b[0mBerkeley\u001b[0m    \u001b[1;36m25.0\u001b[0m    \u001b[1;36m77.0\u001b[0m  \u001b[1;36m298.15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3563\u001b[0m         \"\"\"\n\u001b[1;32m-> 3564\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3566\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   5809\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5810\u001b[0m         \"\"\"\n\u001b[1;32m-> 5811\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5812\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, filter, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "proj.perm4missing(flights, 'CANCELLATION_REASON', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "Filter the dataset `jb_sw` to flights *originating* from the following 10 airports: ABQ, BDL, BUR, DCA, MSY, PBI, PHX, RNO, SJC, SLC.\n",
    "\n",
    "Illustrate Simpson's paradox with this table:\n",
    "* Calculate the proportion of each airline's flights that are delayed (at each of the 10 airports):\n",
    "    - Create a function `prop_delayed_by_airline` that takes in a dataframe like `jb_sw` and returns a DataFrame indexed by airline that contains the proportion of each airline's flights that are delayed.\n",
    "* Calculate these proportions across all airports in the dataset (at each of the 10 airports):\n",
    "    - Create a function `prop_delayed_by_airline_airport` that takes in a dataframe like `jb_sw` and returns a DataFrame, with columns given by airports, indexed by airline, that contains the proportion of each airline's flights that are delayed at each airport.\n",
    "\n",
    "*Remark 1:* For the purpose of this question, a canceled flight is **not** considered delayed.\n",
    "\n",
    "*Remark 2:* Make sure that the functions only work with the columns that are absolutely necessary for the question to avoid out of memory errors!\n",
    "\n",
    "Verify that Simpson's paradox is present in this output! \n",
    "\n",
    "Can you explain *why* Simpson's paradox is occurring? (Hint: where are these airports located? Which have the most flights?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**\n",
    "\n",
    "Your work above illustrates Simpson's paradox on the specific dataset of flights originating from 10 specific airports. However, this still requires you to look at two dataframe to see if the paradox is present. Now, you will create a function that verifies Simpson's paradox in general. You will do this by writing code to compare the two dataframes, instead of inspecting them manually.\n",
    "\n",
    "Create a function `verify_simpson` that returns a boolean output regarding if the paradox is present.\n",
    "```\n",
    "verify_simpson(df, group1, group2, occur)\n",
    "```\n",
    "- df is a dataframe (e.g. jb_sw),\n",
    "- group1 is the first group being aggregated against (e.g. `AIRLINE`),\n",
    "- group2 is the second group being aggregated against (e.g. `ORIGIN_AIRPORT`),\n",
    "- occur is a column with values {0, 1}, denoting if an event occurred for that individual.\n",
    "  (e.g. \"1 if flight was delayed\" and \"0 if flight was not delayed\")\n",
    "\n",
    "`verify_simpson` should return `True` only if there is a reversal for *every* value of `group2` (e.g. for every airport).\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider the following dataframe `df` with columns `treatment`, `stone_size`, and `success`:\n",
    "\n",
    "|treatment|stone_size|success|\n",
    "|---|---|---|\n",
    "|A|small|1|\n",
    "|B|small|1|\n",
    "|...|...|...|\n",
    "|A|large|0|\n",
    "|B|small|0|\n",
    "|B|small|1|\n",
    "\n",
    "`df` corresponds to the following diagram:\n",
    "<img src=\"https://miro.medium.com/max/996/1*IfVjfdGD7RPwLDC6WzT9Uw.png\" style=\"width: 300px\"/>\n",
    "\n",
    "Here, `verify_simpson(df, 'treatment', 'stone_size', 'success')` should return `True`.\n",
    "\n",
    "Verify that you function works on the previous question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a==a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.NaN==np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus problem (worth zero points)\n",
    "\n",
    "This question is for fun and explores a very data-science type problem: can we automate finding examples of Simpson's Paradox? This is an active area of research (see for example: https://arxiv.org/pdf/1801.04385.pdf), but is a very accessible problem. While totally optional, this question can lead to pretty interesting self-driven projects!\n",
    "\n",
    "**Question 10 (Searching for Simpson's Paradox):**\n",
    "\n",
    "As you observed from the reading in the lecture notes, Simpson's Paradox often occurs due to some confounding factor among the columns of a dataset. In the case of gender balance in student admissions to academic departments at UC Berkeley, the confounding factor was the admission rate (i.e. how hard it is to gain admission to a department).\n",
    "\n",
    "What might be a confounding factor be for flight delays among airports in question 8? Now you are going to write code to discover instances of Simpson's Paradox; that is, you will programmatically find interesting relationships present in the data.\n",
    "\n",
    "Given the dataset `jb_sw`, we'd like to find new groups of airports, as in question 8, for which the statistics of flight delays between JetBlue and Southwest satisfy Simpson's Paradox.\n",
    "\n",
    "Create a function `search_simpsons` that takes in the `jb_sw` dataset and a number `N`, and returns a list of `N` airports for which the proportion of flight delays between JetBlue and Southwest satisfies Simpson's Paradox.\n",
    "- Only consider airports that have '3 letter codes',\n",
    "- Only consider airports that have at least one JetBlue *and* Southwest flight.\n",
    "\n",
    "*Remark 1:* Iterate through groups of airports of size `N` using `itertools.combinations` until you find a group that works. Make sure your function finishes, even if it doesn't find something.\n",
    "\n",
    "*Remark 2:* You should be using your work from Question 9!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you finished the project!\n",
    "\n",
    "### Before you submit:\n",
    "* Be sure you run the doctests on all your code in project02.py\n",
    "\n",
    "### To submit:\n",
    "* **Upload the .py file to gradescope**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
